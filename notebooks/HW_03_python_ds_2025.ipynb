{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca0353b",
   "metadata": {
    "id": "aca0353b"
   },
   "source": [
    "# Домашнее задание 3. Парсинг, Git и тестирование на Python\n",
    "\n",
    "**Цели задания:**\n",
    "\n",
    "* Освоить базовые подходы к web-scraping с библиотеками `requests` и `BeautisulSoup`: навигация по страницам, извлечение HTML-элементов, парсинг.\n",
    "* Научиться автоматизировать задачи с использованием библиотеки `schedule`.\n",
    "* Попрактиковаться в использовании Git и оформлении проектов на GitHub.\n",
    "* Написать и запустить простые юнит-тесты с использованием `pytest`.\n",
    "\n",
    "\n",
    "В этом домашнем задании вы разработаете систему для автоматического сбора данных о книгах с сайта [Books to Scrape](http://books.toscrape.com). Нужно реализовать функции для парсинга всех страниц сайта, извлечения информации о книгах, автоматического ежедневного запуска задачи и сохранения результата.\n",
    "\n",
    "Важной частью задания станет оформление проекта: вы создадите репозиторий на GitHub, оформите `README.md`, добавите артефакты (код, данные, отчеты) и напишете базовые тесты на `pytest`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "K3JMV0qwmA_q",
   "metadata": {
    "id": "K3JMV0qwmA_q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\perfe\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 schedule pytest  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873d4904",
   "metadata": {
    "id": "873d4904"
   },
   "outputs": [],
   "source": [
    "# Библиотеки, которые могут вам понадобиться\n",
    "# При необходимости расширяйте список\n",
    "# Импортирт всех необходимых бибилиотек\n",
    "import time\n",
    "import requests\n",
    "import schedule\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unTvsWaegHdj",
   "metadata": {
    "id": "unTvsWaegHdj"
   },
   "source": [
    "## Задание 1. Сбор данных об одной книге (20 баллов)\n",
    "\n",
    "В этом задании мы начнем подготовку скрипта для парсинга информации о книгах со страниц каталога сайта [Books to Scrape](https://books.toscrape.com/).\n",
    "\n",
    "Для начала реализуйте функцию `get_book_data`, которая будет получать данные о книге с одной страницы (например, с [этой](http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html)). Соберите всю информацию, включая название, цену, рейтинг, количество в наличии, описание и дополнительные характеристики из таблицы Product Information. Результат достаточно вернуть в виде словаря.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8** — помимо качественно написанного кода важно также документировать функции по стандарту:\n",
    "* кратко описать, что она делает и для чего нужна;\n",
    "* какие входные аргументы принимает, какого они типа и что означают по смыслу;\n",
    "* аналогично описать возвращаемые значения.\n",
    "\n",
    "*P. S. Состав, количество аргументов функции и тип возвращаемого значения можете менять как вам удобно. То, что написано ниже в шаблоне — лишь пример.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "UfD2vAjHkEoS",
   "metadata": {
    "id": "UfD2vAjHkEoS"
   },
   "outputs": [],
   "source": [
    "def get_book_data(book_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Парсит данные о книге с указанного URL.\n",
    "    \n",
    "    Args:\n",
    "        book_url (str): URL страницы книги для парсинга\n",
    "        \n",
    "    Returns:\n",
    "        dict: Словарь с информацией о книге\n",
    "    \"\"\"\n",
    "\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    try:\n",
    "        response = requests.get(book_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Название книги\n",
    "        title = soup.find('h1').text.strip()\n",
    "        \n",
    "        # Цена\n",
    "        price = soup.find('p', class_='price_color').text.strip()\n",
    "        \n",
    "        # Рейтинг\n",
    "        rating_class = soup.find('p', class_='star-rating')['class'][1]\n",
    "        rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
    "        rating = rating_map.get(rating_class, 0)\n",
    "        \n",
    "        # Количество в наличии\n",
    "        stock_text = soup.find('p', class_='instock availability').text.strip()\n",
    "        stock_match = re.search(r'\\((\\d+) available\\)', stock_text)\n",
    "        stock = int(stock_match.group(1)) if stock_match else 0\n",
    "        \n",
    "        # Описание\n",
    "        description_element = soup.find('div', id='product_description')\n",
    "        description = \"\"\n",
    "        if description_element:\n",
    "            description_sibling = description_element.find_next_sibling('p')\n",
    "            if description_sibling:\n",
    "                description = description_sibling.text.strip()\n",
    "        \n",
    "        # Таблица Product Information\n",
    "        product_info = {}\n",
    "        table = soup.find('table', class_='table table-striped')\n",
    "        if table:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                header = row.find('th').text.strip()\n",
    "                value = row.find('td').text.strip()\n",
    "                product_info[header] = value\n",
    "        \n",
    "        book_data = {\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'rating': rating,\n",
    "            'stock': stock,\n",
    "            'description': description,\n",
    "            'upc': product_info.get('UPC', ''),\n",
    "            'product_type': product_info.get('Product Type', ''),\n",
    "            'price_excl_tax': product_info.get('Price (excl. tax)', ''),\n",
    "            'price_incl_tax': product_info.get('Price (incl. tax)', ''),\n",
    "            'tax': product_info.get('Tax', ''),\n",
    "            'availability': product_info.get('Availability', ''),\n",
    "            'num_reviews': product_info.get('Number of reviews', '')\n",
    "        }\n",
    "        \n",
    "        return book_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при парсинге {book_url}: {e}\")\n",
    "        return {}\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moRSO9Itp1LT",
   "metadata": {
    "id": "moRSO9Itp1LT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'A Light in the Attic',\n",
       " 'price': '£51.77',\n",
       " 'rating': 3,\n",
       " 'stock': 22,\n",
       " 'description': \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\",\n",
       " 'upc': 'a897fe39b1053632',\n",
       " 'product_type': 'Books',\n",
       " 'price_excl_tax': '£51.77',\n",
       " 'price_incl_tax': '£51.77',\n",
       " 'tax': '£0.00',\n",
       " 'availability': 'In stock (22 available)',\n",
       " 'num_reviews': '0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Используйте для самопроверки\n",
    "book_url = 'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "get_book_data(book_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u601Q4evosq6",
   "metadata": {
    "id": "u601Q4evosq6"
   },
   "source": [
    "## Задание 2. Сбор данных обо всех книгах (20 баллов)\n",
    "\n",
    "Создайте функцию `scrape_books`, которая будет проходиться по всем страницам из каталога (вида `http://books.toscrape.com/catalogue/page-{N}.html`) и осуществлять парсинг всех страниц в цикле, используя ранее написанную `get_book_data`.\n",
    "\n",
    "Добавьте аргумент-флаг, который будет отвечать за сохранение результата в файл: если он будет равен `True`, то информация сохранится в ту же папку в файл `books_data.txt`; иначе шаг сохранения будет пропущен.\n",
    "\n",
    "**Также не забывайте про соблюдение PEP-8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kk78l6oDkdxl",
   "metadata": {
    "id": "kk78l6oDkdxl"
   },
   "outputs": [],
   "source": [
    "def scrape_books(is_save: bool = False, max_pages: int = None) -> list:\n",
    "    \"\"\"\n",
    "    Parses books from catalog pages.\n",
    "    \n",
    "    Args:\n",
    "        is_save (bool): If True, saves data to books_data.txt file\n",
    "        max_pages (int, optional): Maximum number of pages to parse.\n",
    "                                  If None, parses all pages.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries with book information\n",
    "    \"\"\"\n",
    "\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import time\n",
    "    \n",
    "    base_url = \"http://books.toscrape.com/catalogue/\"\n",
    "    all_books = []\n",
    "    page = 1\n",
    "    \n",
    "    print(\"Starting parsing...\")\n",
    "    if max_pages:\n",
    "        print(f\"Limit: {max_pages} pages\")\n",
    "    else:\n",
    "        print(\"Parsing all available pages\")\n",
    "    \n",
    "    while True:\n",
    "        # Проверка лимита страниц\n",
    "        if max_pages and page > max_pages:\n",
    "            print(f\"Reached limit of {max_pages} pages\")\n",
    "            break\n",
    "            \n",
    "        if page == 1:\n",
    "            url = f\"{base_url}page-1.html\"\n",
    "        else:\n",
    "            url = f\"{base_url}page-{page}.html\"\n",
    "        \n",
    "        print(f\"Parsing page {page}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            # Если страницы не существует, заканчивает цикл\n",
    "            if response.status_code == 404:\n",
    "                print(f\"Reached last page ({page-1} pages total)\")\n",
    "                break\n",
    "                \n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            books = soup.find_all('article', class_='product_pod')\n",
    "            \n",
    "            if not books:\n",
    "                print(\"No books found on page\")\n",
    "                break\n",
    "            \n",
    "            print(f\"Found {len(books)} books on page {page}\")\n",
    "            \n",
    "            book_count = 0\n",
    "            for book in books:\n",
    "                # Get link to book page\n",
    "                book_link = book.find('h3').find('a')['href']\n",
    "                \n",
    "                if book_link.startswith('../../../'):\n",
    "                    full_book_url = f\"http://books.toscrape.com/catalogue/{book_link[9:]}\"\n",
    "                else:\n",
    "                    full_book_url = f\"http://books.toscrape.com/catalogue/{book_link}\"\n",
    "                \n",
    "                book_data = get_book_data(full_book_url)\n",
    "                if book_data:\n",
    "                    all_books.append(book_data)\n",
    "                    book_count += 1\n",
    "                \n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            print(f\"Page {page}: successfully collected {book_count} books\")\n",
    "            \n",
    "            next_button = soup.find('li', class_='next')\n",
    "            if not next_button:\n",
    "                print(f\"No more pages to parse (total {page} pages)\")\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Network error while parsing page {page}: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error while parsing page {page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Parsing completed. Total collected {len(all_books)} books from {page-1} pages\")\n",
    "    \n",
    "    # Сохранение в файл\n",
    "    if is_save and all_books:\n",
    "        try:\n",
    "            with open('books_data.txt', 'w', encoding='utf-8') as f:\n",
    "                f.write(\"BOOKS DATA\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\")\n",
    "                f.write(f\"Total books: {len(all_books)}\\n\")\n",
    "                f.write(f\"Total pages: {page-1}\\n\")\n",
    "                f.write(f\"Collection time: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "                \n",
    "                for i, book in enumerate(all_books, 1):\n",
    "                    f.write(f\"BOOK #{i}\\n\")\n",
    "                    f.write(f\"Title: {book.get('title', 'N/A')}\\n\")\n",
    "                    f.write(f\"Price: {book.get('price', 'N/A')}\\n\")\n",
    "                    f.write(f\"Rating: {book.get('rating', 'N/A')}/5\\n\")\n",
    "                    f.write(f\"Stock: {book.get('stock', 'N/A')} items\\n\")\n",
    "                    f.write(f\"Description: {book.get('description', 'N/A')}\\n\")\n",
    "                    f.write(f\"UPC: {book.get('upc', 'N/A')}\\n\")\n",
    "                    f.write(f\"Product Type: {book.get('product_type', 'N/A')}\\n\")\n",
    "                    f.write(f\"Price (excl. tax): {book.get('price_excl_tax', 'N/A')}\\n\")\n",
    "                    f.write(f\"Price (incl. tax): {book.get('price_incl_tax', 'N/A')}\\n\")\n",
    "                    f.write(f\"Tax: {book.get('tax', 'N/A')}\\n\")\n",
    "                    f.write(f\"Availability: {book.get('availability', 'N/A')}\\n\")\n",
    "                    f.write(f\"Number of reviews: {book.get('num_reviews', 'N/A')}\\n\")\n",
    "                    f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            print(f\"Full data saved to books_data.txt\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving file: {e}\")\n",
    "    \n",
    "    return all_books\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Bt7mrXcbkj5Q",
   "metadata": {
    "id": "Bt7mrXcbkj5Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parsing...\n",
      "Parsing all available pages\n",
      "Parsing page 1: http://books.toscrape.com/catalogue/page-1.html\n",
      "Found 20 books on page 1\n",
      "Page 1: successfully collected 20 books\n",
      "Parsing page 2: http://books.toscrape.com/catalogue/page-2.html\n",
      "Found 20 books on page 2\n",
      "Page 2: successfully collected 20 books\n",
      "Parsing page 3: http://books.toscrape.com/catalogue/page-3.html\n",
      "Found 20 books on page 3\n",
      "Page 3: successfully collected 20 books\n",
      "Parsing page 4: http://books.toscrape.com/catalogue/page-4.html\n",
      "Found 20 books on page 4\n",
      "Page 4: successfully collected 20 books\n",
      "Parsing page 5: http://books.toscrape.com/catalogue/page-5.html\n",
      "Found 20 books on page 5\n",
      "Page 5: successfully collected 20 books\n",
      "Parsing page 6: http://books.toscrape.com/catalogue/page-6.html\n",
      "Found 20 books on page 6\n",
      "Page 6: successfully collected 20 books\n",
      "Parsing page 7: http://books.toscrape.com/catalogue/page-7.html\n",
      "Found 20 books on page 7\n",
      "Page 7: successfully collected 20 books\n",
      "Parsing page 8: http://books.toscrape.com/catalogue/page-8.html\n",
      "Found 20 books on page 8\n",
      "Page 8: successfully collected 20 books\n",
      "Parsing page 9: http://books.toscrape.com/catalogue/page-9.html\n",
      "Found 20 books on page 9\n",
      "Page 9: successfully collected 20 books\n",
      "Parsing page 10: http://books.toscrape.com/catalogue/page-10.html\n",
      "Found 20 books on page 10\n",
      "Page 10: successfully collected 20 books\n",
      "Parsing page 11: http://books.toscrape.com/catalogue/page-11.html\n",
      "Found 20 books on page 11\n",
      "Page 11: successfully collected 20 books\n",
      "Parsing page 12: http://books.toscrape.com/catalogue/page-12.html\n",
      "Found 20 books on page 12\n",
      "Page 12: successfully collected 20 books\n",
      "Parsing page 13: http://books.toscrape.com/catalogue/page-13.html\n",
      "Found 20 books on page 13\n",
      "Page 13: successfully collected 20 books\n",
      "Parsing page 14: http://books.toscrape.com/catalogue/page-14.html\n",
      "Found 20 books on page 14\n",
      "Page 14: successfully collected 20 books\n",
      "Parsing page 15: http://books.toscrape.com/catalogue/page-15.html\n",
      "Found 20 books on page 15\n",
      "Page 15: successfully collected 20 books\n",
      "Parsing page 16: http://books.toscrape.com/catalogue/page-16.html\n",
      "Found 20 books on page 16\n",
      "Page 16: successfully collected 20 books\n",
      "Parsing page 17: http://books.toscrape.com/catalogue/page-17.html\n",
      "Found 20 books on page 17\n",
      "Page 17: successfully collected 20 books\n",
      "Parsing page 18: http://books.toscrape.com/catalogue/page-18.html\n",
      "Found 20 books on page 18\n",
      "Page 18: successfully collected 20 books\n",
      "Parsing page 19: http://books.toscrape.com/catalogue/page-19.html\n",
      "Found 20 books on page 19\n",
      "Page 19: successfully collected 20 books\n",
      "Parsing page 20: http://books.toscrape.com/catalogue/page-20.html\n",
      "Found 20 books on page 20\n",
      "Page 20: successfully collected 20 books\n",
      "Parsing page 21: http://books.toscrape.com/catalogue/page-21.html\n",
      "Found 20 books on page 21\n",
      "Page 21: successfully collected 20 books\n",
      "Parsing page 22: http://books.toscrape.com/catalogue/page-22.html\n",
      "Found 20 books on page 22\n",
      "Page 22: successfully collected 20 books\n",
      "Parsing page 23: http://books.toscrape.com/catalogue/page-23.html\n",
      "Found 20 books on page 23\n",
      "Page 23: successfully collected 20 books\n",
      "Parsing page 24: http://books.toscrape.com/catalogue/page-24.html\n",
      "Found 20 books on page 24\n",
      "Page 24: successfully collected 20 books\n",
      "Parsing page 25: http://books.toscrape.com/catalogue/page-25.html\n",
      "Found 20 books on page 25\n",
      "Page 25: successfully collected 20 books\n",
      "Parsing page 26: http://books.toscrape.com/catalogue/page-26.html\n",
      "Found 20 books on page 26\n",
      "Page 26: successfully collected 20 books\n",
      "Parsing page 27: http://books.toscrape.com/catalogue/page-27.html\n",
      "Found 20 books on page 27\n",
      "Page 27: successfully collected 20 books\n",
      "Parsing page 28: http://books.toscrape.com/catalogue/page-28.html\n",
      "Found 20 books on page 28\n",
      "Page 28: successfully collected 20 books\n",
      "Parsing page 29: http://books.toscrape.com/catalogue/page-29.html\n",
      "Found 20 books on page 29\n",
      "Page 29: successfully collected 20 books\n",
      "Parsing page 30: http://books.toscrape.com/catalogue/page-30.html\n",
      "Found 20 books on page 30\n",
      "Page 30: successfully collected 20 books\n",
      "Parsing page 31: http://books.toscrape.com/catalogue/page-31.html\n",
      "Found 20 books on page 31\n",
      "Page 31: successfully collected 20 books\n",
      "Parsing page 32: http://books.toscrape.com/catalogue/page-32.html\n",
      "Found 20 books on page 32\n",
      "Page 32: successfully collected 20 books\n",
      "Parsing page 33: http://books.toscrape.com/catalogue/page-33.html\n",
      "Found 20 books on page 33\n",
      "Page 33: successfully collected 20 books\n",
      "Parsing page 34: http://books.toscrape.com/catalogue/page-34.html\n",
      "Found 20 books on page 34\n",
      "Page 34: successfully collected 20 books\n",
      "Parsing page 35: http://books.toscrape.com/catalogue/page-35.html\n",
      "Found 20 books on page 35\n",
      "Page 35: successfully collected 20 books\n",
      "Parsing page 36: http://books.toscrape.com/catalogue/page-36.html\n",
      "Found 20 books on page 36\n",
      "Page 36: successfully collected 20 books\n",
      "Parsing page 37: http://books.toscrape.com/catalogue/page-37.html\n",
      "Found 20 books on page 37\n",
      "Page 37: successfully collected 20 books\n",
      "Parsing page 38: http://books.toscrape.com/catalogue/page-38.html\n",
      "Found 20 books on page 38\n",
      "Page 38: successfully collected 20 books\n",
      "Parsing page 39: http://books.toscrape.com/catalogue/page-39.html\n",
      "Found 20 books on page 39\n",
      "Page 39: successfully collected 20 books\n",
      "Parsing page 40: http://books.toscrape.com/catalogue/page-40.html\n",
      "Found 20 books on page 40\n",
      "Page 40: successfully collected 20 books\n",
      "Parsing page 41: http://books.toscrape.com/catalogue/page-41.html\n",
      "Found 20 books on page 41\n",
      "Page 41: successfully collected 20 books\n",
      "Parsing page 42: http://books.toscrape.com/catalogue/page-42.html\n",
      "Found 20 books on page 42\n",
      "Page 42: successfully collected 20 books\n",
      "Parsing page 43: http://books.toscrape.com/catalogue/page-43.html\n",
      "Found 20 books on page 43\n",
      "Page 43: successfully collected 20 books\n",
      "Parsing page 44: http://books.toscrape.com/catalogue/page-44.html\n",
      "Found 20 books on page 44\n",
      "Page 44: successfully collected 20 books\n",
      "Parsing page 45: http://books.toscrape.com/catalogue/page-45.html\n",
      "Found 20 books on page 45\n",
      "Page 45: successfully collected 20 books\n",
      "Parsing page 46: http://books.toscrape.com/catalogue/page-46.html\n",
      "Found 20 books on page 46\n",
      "Page 46: successfully collected 20 books\n",
      "Parsing page 47: http://books.toscrape.com/catalogue/page-47.html\n",
      "Found 20 books on page 47\n",
      "Page 47: successfully collected 20 books\n",
      "Parsing page 48: http://books.toscrape.com/catalogue/page-48.html\n",
      "Found 20 books on page 48\n",
      "Page 48: successfully collected 20 books\n",
      "Parsing page 49: http://books.toscrape.com/catalogue/page-49.html\n",
      "Found 20 books on page 49\n",
      "Page 49: successfully collected 20 books\n",
      "Parsing page 50: http://books.toscrape.com/catalogue/page-50.html\n",
      "Found 20 books on page 50\n",
      "Page 50: successfully collected 20 books\n",
      "No more pages to parse (total 50 pages)\n",
      "Parsing completed. Total collected 1000 books from 49 pages\n",
      "Full data saved to books_data.txt\n",
      "<class 'list'> 1000\n"
     ]
    }
   ],
   "source": [
    "# Проверка работоспособности функции\n",
    "res = scrape_books(is_save=True, max_pages=None)\n",
    "print(type(res), len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5fd728nl8a8",
   "metadata": {
    "id": "z5fd728nl8a8"
   },
   "source": [
    "## Задание 3. Настройка регулярной выгрузки (10 баллов)\n",
    "\n",
    "Настройте автоматический запуск функции сбора данных каждый день в 19:00.\n",
    "Для автоматизации используйте библиотеку `schedule`. Функция должна запускаться в указанное время и сохранять обновленные данные в текстовый файл.\n",
    "\n",
    "\n",
    "\n",
    "Бесконечный цикл должен обеспечивать постоянное ожидание времени для запуска задачи и выполнять ее по расписанию. Однако чтобы не перегружать систему, стоит подумать о том, чтобы выполнять проверку нужного времени не постоянно, а раз в какой-то промежуток. В этом вам может помочь `time.sleep(...)`.\n",
    "\n",
    "Проверьте работоспособность кода локально на любом времени чч:мм.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "SajRRCj4n8BZ",
   "metadata": {
    "id": "SajRRCj4n8BZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ПЛАНИРОВЩИК: Инициализирован\n",
      "✓ Задача: Ежедневный парсинг в 19:00\n",
      "✓ Текущее время: 15:45:23\n",
      "✓ Планировщик настроен, но НЕ запущен\n"
     ]
    }
   ],
   "source": [
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "import time\n",
    "import schedule\n",
    "\n",
    "def scheduled_scraping():\n",
    "    \"\"\"\n",
    "    Функция для автоматического запуска парсинга по расписанию.\n",
    "    Будет вызываться только в 19:00 каждый день.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"АВТОМАТИЧЕСКИЙ ПАРСИНГ: Запуск в {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Здесь будет запуск scrape_books(is_save=True, max_pages=None)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Настройка расписания - ежедневно в 19:00\n",
    "schedule.every().day.at(\"19:00\").do(scheduled_scraping)\n",
    "\n",
    "print(\"✓ ПЛАНИРОВЩИК: Инициализирован\")\n",
    "print(\"✓ Задача: Ежедневный парсинг в 19:00\")\n",
    "print(f\"✓ Текущее время: {time.strftime('%H:%M:%S')}\")\n",
    "print(\"✓ Планировщик настроен, но НЕ запущен\")\n",
    "\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFiPtEyaoLxq",
   "metadata": {
    "id": "XFiPtEyaoLxq"
   },
   "source": [
    "## Задание 4. Написание автотестов (15 баллов)\n",
    "\n",
    "Создайте минимум три автотеста для ключевых функций парсинга — например, `get_book_data` и `scrape_books`. Идеи проверок (можете использовать свои):\n",
    "\n",
    "* данные о книге возвращаются в виде словаря с нужными ключами;\n",
    "* список ссылок или количество собранных книг соответствует ожиданиям;\n",
    "* значения отдельных полей (например, `title`) корректны.\n",
    "\n",
    "Оформите тесты в отдельном скрипте `tests/test_scraper.py`, используйте библиотеку `pytest`. Убедитесь, что тесты проходят успешно при запуске из терминала командой `pytest`.\n",
    "\n",
    "Также выведите результат их выполнения в ячейке ниже.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lBFAw4b3z8QY",
   "metadata": {
    "id": "lBFAw4b3z8QY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текущая директория: C:\\Users\\perfe\\OneDrive\\Рабочий стол\\Projects\\panfilova_tatiana_hw3\n",
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.10.0, pytest-8.4.2, pluggy-1.6.0 -- C:\\Users\\perfe\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\perfe\\OneDrive\\Рабочий стол\\Projects\\panfilova_tatiana_hw3\n",
      "plugins: anyio-4.11.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "tests/scraper_test.py::test_get_book_data_returns_dict \u001b[32mPASSED\u001b[0m\u001b[32m            [ 33%]\u001b[0m\n",
      "tests/scraper_test.py::test_get_book_data_has_required_keys \u001b[32mPASSED\u001b[0m\u001b[32m       [ 66%]\u001b[0m\n",
      "tests/scraper_test.py::test_scrape_books_returns_books \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 15.62s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ROOT = r\"C:\\Users\\perfe\\OneDrive\\Рабочий стол\\Projects\\panfilova_tatiana_hw3\"\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "\n",
    "try:\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    print(\"Текущая директория:\", os.getcwd())\n",
    "    \n",
    "    !pytest tests/scraper_test.py -v\n",
    "finally:\n",
    "    os.chdir(original_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e312e5b-d2d3-4b6a-a3a7-b4e619a3ddd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cRSQlHfRtOdN",
   "metadata": {
    "id": "cRSQlHfRtOdN"
   },
   "source": [
    "## Задание 5. Оформление проекта на GitHub и работа с Git (35 баллов)\n",
    "\n",
    "В этом задании нужно воспользоваться системой контроля версий Git и платформой GitHub для хранения и управления своим проектом. **Ссылку на свой репозиторий пришлите в форме для сдачи ответа.**\n",
    "\n",
    "### Пошаговая инструкция и задания\n",
    "\n",
    "**1. Установите Git на свой компьютер.**\n",
    "\n",
    "* Для Windows: [скачайте установщик](https://git-scm.com/downloads) и выполните установку.\n",
    "* Для macOS:\n",
    "\n",
    "  ```\n",
    "  brew install git\n",
    "  ```\n",
    "* Для Linux:\n",
    "\n",
    "  ```\n",
    "  sudo apt update\n",
    "  sudo apt install git\n",
    "  ```\n",
    "\n",
    "**2. Настройте имя пользователя и email.**\n",
    "\n",
    "Это нужно для подписи ваших коммитов, сделайте в терминале через `git config ...`.\n",
    "\n",
    "**3. Создайте аккаунт на GitHub**, если у вас его еще нет:\n",
    "[https://github.com](https://github.com)\n",
    "\n",
    "**4. Создайте новый репозиторий на GitHub:**\n",
    "\n",
    "* Найдите кнопку **New repository**.\n",
    "* Укажите название, краткое описание, выберите тип **Public** (чтобы мы могли проверить ДЗ).\n",
    "* Не ставьте галочку Initialize this repository with a README.\n",
    "\n",
    "**5. Создайте локальную папку с проектом.** Можно в терминале, можно через UI, это не имеет значения.\n",
    "\n",
    "**6. Инициализируйте Git в этой папке.** Здесь уже придется воспользоваться некоторой командой в терминале.\n",
    "\n",
    "**7. Привяжите локальный репозиторий к удаленному на GitHub.**\n",
    "\n",
    "**8. Создайте ветку разработки.** По умолчанию вы будете находиться в ветке `main`, создайте и переключитесь на ветку `hw-books-parser`.\n",
    "\n",
    "**9. Добавьте в проект следующие файлы и папки:**\n",
    "\n",
    "* `scraper.py` — ваш основной скрипт для сбора данных.\n",
    "* `README.md` — файл с кратким описанием проекта:\n",
    "\n",
    "  * цель;\n",
    "  * инструкции по запуску;\n",
    "  * список используемых библиотек.\n",
    "* `requirements.txt` — файл со списком зависимостей, необходимых для проекта (не присылайте все из глобального окружения, создайте изолированную виртуальную среду, добавьте в нее все нужное для проекта и получите список библиотек через `pip freeze`).\n",
    "* `artifacts/` — папка с результатами парсинга (`books_data.txt` — полностью или его часть, если весь не поместится на GitHub).\n",
    "* `notebooks/` — папка с заполненным ноутбуком `HW_03_python_ds_2025.ipynb` и запущенными ячейками с выводами на экран.\n",
    "* `tests/` — папка с тестами на `pytest`, оформите их в формате скрипта(-ов) с расширением `.py`.\n",
    "* `.gitignore` — стандартный файл, который позволит исключить временные файлы при добавлении в отслеживаемые (например, `__pycache__/`, `.DS_Store`, `*.pyc`, `venv/` и др.).\n",
    "\n",
    "\n",
    "**10. Сделайте коммит.**\n",
    "\n",
    "**11. Отправьте свою ветку на GitHub.**\n",
    "\n",
    "**12. Создайте Pull Request:**\n",
    "\n",
    "* Перейдите в репозиторий на GitHub.\n",
    "* Нажмите кнопку **Compare & pull request**.\n",
    "* Укажите, что было добавлено, и нажмите **Create pull request**.\n",
    "\n",
    "**13. Выполните слияние Pull Request:**\n",
    "\n",
    "* Убедитесь, что нет конфликтов.\n",
    "* Нажмите **Merge pull request**, затем **Confirm merge**.\n",
    "\n",
    "**14. Скачайте изменения из основной ветки локально.**\n",
    "\n",
    "\n",
    "\n",
    "### Требования к итоговому репозиторию\n",
    "\n",
    "* Файл `scraper.py` с рабочим кодом парсера.\n",
    "* `README.md` с описанием проекта и инструкцией по запуску.\n",
    "* Папка `artifacts/` с результатом сбора данных (`.txt` файл).\n",
    "* Папка `tests/` с тестами на `pytest`.\n",
    "* Папка `notebooks/` с заполненным ноутбуком `HW_03_python_ds_2025.ipynb`.\n",
    "* Pull Request с комментарием из ветки `hw-books-parser` в ветку `main`.\n",
    "* Примерная структура:\n",
    "\n",
    "  ```\n",
    "  books_scraper/\n",
    "  ├── artifacts/\n",
    "  │   └── books_data.txt\n",
    "  ├── notebooks/\n",
    "  │   └── HW_03_python_ds_2025.ipynb\n",
    "  ├── scraper.py\n",
    "  ├── README.md\n",
    "  ├── tests/\n",
    "  │   └── test_scraper.py\n",
    "  ├── .gitignore\n",
    "  └── requirements.txt\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692d94c-8985-4d2d-8871-590aa477d7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
